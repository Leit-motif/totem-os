"""Typer-based CLI for Totem OS."""

import logging
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import typer
from rich.console import Console
from rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table

from .capture import ingest_file_capture, ingest_text_capture
from .config import TotemConfig
from .distill import (
    load_routed_items,
    process_distillation,
    process_distillation_dry_run,
    undo_canon_write,
)
from .ingest_manifest import (
    IngestCounts,
    IngestErrorItem,
    append_manifest_record,
    build_manifest_record,
    latest_record_by_source,
    load_manifest_records,
    totals_by_source,
    try_get_git_sha,
)
from .ledger import LedgerWriter, read_ledger_tail
from .llm import get_llm_client
from .paths import VaultPaths
from .review import (
    KeyInputSource,
    LearningEventLogger,
    ReviewQueue,
    ReviewSession,
    load_or_create_proposals,
)
from .route import process_capture_routing
from .chatgpt.local_ingest import (
    LocalIngestError,
    ingest_from_downloads_with_summary,
    ingest_from_zip_with_summary,
)
from .omi.ingest import OmiIngestCrash, sync_omi_transcripts
from .agents.intent_arbiter import IntentArbiterAgent
from .models.intent import IntentType

# Global vault path storage
_global_vault_path = None

def set_global_vault_path(vault_path: str = typer.Option(
    None,
    "--totem-vault",
    help="Path to Totem vault directory (default: auto-resolve from current directory or repo config)",
)):
    """Global vault path option callback."""
    global _global_vault_path
    _global_vault_path = vault_path

def get_global_vault_path():
    """Get the global vault path set by CLI option."""
    return _global_vault_path

app = typer.Typer(
    name="totem",
    help="Totem OS - Local-first personal cognitive operating system",
    add_completion=False,
    callback=set_global_vault_path,
)



console = Console()

daemon_app = typer.Typer(help="Daemon vault commands", add_completion=False)
app.add_typer(daemon_app, name="daemon")

def _parse_iso_ts(ts: str) -> datetime:
    """Parse ISO timestamp with optional Z suffix."""
    if ts.endswith("Z"):
        ts = ts.replace("Z", "+00:00")
    return datetime.fromisoformat(ts)


@app.command()
def link_vault(
    vault_path: str = typer.Argument(..., help="Path to existing Totem vault directory"),
):
    """Link an existing vault to the current repository.

    Creates .totem/config.toml in the repository root with the vault path.
    This allows running Totem commands from anywhere within the repo.
    """
    from .config import _find_repo_root, _has_vault_markers

    vault_path = Path(vault_path).resolve()

    # Validate vault exists and has markers
    if not vault_path.exists():
        console.print(f"[red]Error: Vault path does not exist: {vault_path}[/red]")
        raise typer.Exit(code=1)

    if not _has_vault_markers(vault_path):
        console.print(f"[red]Error: Path exists but is not a valid Totem vault: {vault_path}[/red]")
        console.print("[yellow]Vault must contain 90_system/config.yaml[/yellow]")
        raise typer.Exit(code=1)

    # Find repo root
    repo_root = _find_repo_root(Path.cwd())

    # Create .totem directory
    totem_dir = repo_root / ".totem"
    totem_dir.mkdir(exist_ok=True)

    # Write config.toml (untracked; see .totem/config.example.toml)
    config_file = totem_dir / "config.toml"
    config_content = f"""# Totem OS repository configuration (local)
# Auto-generated by 'totem link-vault'
# This file is intentionally untracked; see .totem/config.example.toml

vault_root = "{vault_path}"
"""

    config_file.write_text(config_content, encoding="utf-8")

    console.print(f"[green]✓[/green] Linked vault: {vault_path}")
    console.print(f"[green]✓[/green] Created config: {config_file}")
    console.print(f"[dim]Repository root: {repo_root}[/dim]")
    console.print()
    console.print("[dim]You can now run Totem commands from anywhere in this repository.[/dim]")


@app.command()
def init(
    force: bool = typer.Option(
        False,
        "--force",
        "-f",
        help="Re-initialize even if vault already exists",
    ),
):
    """Initialize a new Totem OS vault with directory structure and system files.

    This command is idempotent - it will not overwrite existing data.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load configuration from environment or use defaults (create_ok mode for init)
    config = TotemConfig.from_env(cli_vault_path=vault_path, mode="create_ok")
    
    vault_root = config.vault_path
    paths = VaultPaths.from_config(config)
    
    # Check if vault already exists
    if vault_root.exists() and not force:
        # Check if it looks like a vault (has system directory)
        if paths.system.exists():
            console.print(f"[yellow]Vault already exists at:[/yellow] {vault_root}")
            console.print("[yellow]Running in idempotent mode - will only create missing items[/yellow]")
        else:
            console.print(f"[yellow]Directory exists but is not a vault:[/yellow] {vault_root}")
            console.print("[yellow]Initializing vault structure...[/yellow]")
    else:
        console.print(f"[green]Initializing new Totem OS vault at:[/green] {vault_root}")
    
    # Create all directories (idempotent - won't fail if exists)
    directories_created = []
    for directory in paths.get_all_directories():
        if not directory.exists():
            directory.mkdir(parents=True, exist_ok=True)
            directories_created.append(directory)
    
    if directories_created:
        console.print(f"[green]+[/green] Created {len(directories_created)} directories")
    else:
        console.print("[dim]All directories already exist[/dim]")
    
    # Create config.yaml if it doesn't exist
    if not paths.config_file.exists():
        paths.config_file.write_text(config.to_yaml_str())
        console.print(f"[green]+[/green] Created config: {paths.config_file}")
    else:
        console.print(f"[dim]Config already exists: {paths.config_file}[/dim]")
    
    # Create empty ledger.jsonl if it doesn't exist
    if not paths.ledger_file.exists():
        paths.ledger_file.touch()
        console.print(f"[green]+[/green] Created ledger: {paths.ledger_file}")
    else:
        console.print(f"[dim]Ledger already exists: {paths.ledger_file}[/dim]")
    
    # Create empty entities.json if it doesn't exist
    if not paths.entities_file.exists():
        paths.entities_file.write_text("[]")
        console.print(f"[green]+[/green] Created entities: {paths.entities_file}")
    else:
        console.print(f"[dim]Entities file already exists: {paths.entities_file}[/dim]")
    
    # Create empty todo.md if it doesn't exist
    if not paths.todo_file.exists():
        todo_template = """# Totem OS - Next Actions

<!--
Max 3 actions at a time.
Format: - [ ] action description
-->

"""
        paths.todo_file.write_text(todo_template)
        console.print(f"[green]+[/green] Created todo: {paths.todo_file}")
    else:
        console.print(f"[dim]Todo file already exists: {paths.todo_file}[/dim]")
    
    # Create empty principles.md if it doesn't exist
    if not paths.principles_file.exists():
        principles_template = """# Personal Principles

<!--
This file captures your evolving principles and values.
Updated by distillation when decisions reveal patterns.
-->

"""
        paths.principles_file.write_text(principles_template)
        console.print(f"[green]+[/green] Created principles: {paths.principles_file}")
    else:
        console.print(f"[dim]Principles file already exists: {paths.principles_file}[/dim]")
    
    console.print()
    console.print("[bold green]Vault initialization complete![/bold green]")
    console.print(f"[dim]Vault location:[/dim] {vault_root.absolute()}")


@app.command()
def capture(
    text: str = typer.Option(
        None,
        "--text",
        "-t",
        help="Capture text content directly",
    ),
    file: str = typer.Option(
        None,
        "--file",
        "-f",
        help="Capture file by copying into vault inbox",
    ),
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for inbox folder (YYYY-MM-DD, default: today)",
    ),
):
    """Capture text or file into the vault inbox.

    Creates raw file + .meta.json sidecar in 00_inbox/YYYY-MM-DD/.
    Appends CAPTURE_INGESTED event to ledger.jsonl.
    """
    # Validate: exactly one of --text or --file must be provided
    if not text and not file:
        console.print("[red]Error: Must provide either --text or --file[/red]")
        raise typer.Exit(code=1)

    if text and file:
        console.print("[red]Error: Cannot provide both --text and --file[/red]")
        raise typer.Exit(code=1)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        if text:
            # Ingest text capture
            raw_path, meta_path, capture_id = ingest_text_capture(
                vault_inbox=paths.inbox,
                text=text,
                ledger_writer=ledger_writer,
                date_str=date_str,
            )
            console.print("[green]Captured text:[/green]")
            console.print(f"  Raw:  {raw_path.relative_to(paths.root)}")
            console.print(f"  Meta: {meta_path.relative_to(paths.root)}")
            console.print(f"  ID:   {capture_id}")
        
        elif file:
            # Ingest file capture
            source_path = Path(file)
            raw_path, meta_path, capture_id = ingest_file_capture(
                vault_inbox=paths.inbox,
                source_file_path=source_path,
                ledger_writer=ledger_writer,
                date_str=date_str,
            )
            console.print("[green]Captured file:[/green]")
            console.print(f"  Raw:  {raw_path.relative_to(paths.root)}")
            console.print(f"  Meta: {meta_path.relative_to(paths.root)}")
            console.print(f"  ID:   {capture_id}")

    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)
    except Exception as e:
        console.print(f"[red]Error during capture: {e}[/red]")
        raise typer.Exit(code=1)


@app.command()
def ingest(
    source: str = typer.Option(
        None,
        "--source",
        "-s",
        help="Ingest source: 'omi' or 'chatgpt'",
    ),
    all_sources: bool = typer.Option(
        False,
        "--all",
        help="Run ingestion for both sources",
    ),
    full_history: bool = typer.Option(
        False,
        "--full-history",
        help="Run a full-history ingestion",
    ),
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for Omi ingestion (YYYY-MM-DD, default: today)",
    ),
    include_action_items: bool = typer.Option(
        False,
        "--include-action-items/--no-include-action-items",
        help="Omi: include action items in daily note block (default: False)",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="ChatGPT: preview mode without writing files",
    ),
    daemon_vault: Optional[str] = typer.Option(
        None,
        "--daemon-vault",
        help="ChatGPT: override daemon Obsidian vault path",
    ),
    tooling_vault: Optional[str] = typer.Option(
        None,
        "--tooling-vault",
        help="ChatGPT: override tooling Obsidian vault path",
    ),
    routing_mode: str = typer.Option(
        "heuristic",
        "--routing-mode",
        help="ChatGPT: routing mode {heuristic,force-daemon,force-tooling}",
    ),
    reclassify: bool = typer.Option(
        False,
        "--reclassify",
        help="ChatGPT: recompute routing even if stored in state",
    ),
):
    """Ingest from Omi and/or ChatGPT with manifest recording."""
    if all_sources and source:
        console.print("[red]Error: Use either --all or --source, not both[/red]")
        raise typer.Exit(code=1)

    if not all_sources and not source:
        console.print("[red]Error: Must provide --source or --all[/red]")
        raise typer.Exit(code=1)

    if source and source not in {"omi", "chatgpt"}:
        console.print("[red]Error: --source must be 'omi' or 'chatgpt'[/red]")
        raise typer.Exit(code=1)

    vault_path = get_global_vault_path()
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)
    records = load_manifest_records(paths)
    latest_by_source = latest_record_by_source(records)

    from . import __version__
    from .config import _find_repo_root

    git_sha = try_get_git_sha(_find_repo_root(Path.cwd()))

    def _append_record(record):
        append_manifest_record(paths, record)
        console.print(f"[green]✓[/green] Manifest updated: {paths.ingest_manifest_file}")

    if all_sources or source == "omi":
        resume_from = None
        if full_history:
            last = latest_by_source.get("omi")
            if last and last.last_successful_item_timestamp:
                resume_from = _parse_iso_ts(last.last_successful_item_timestamp)

        try:
            summary = sync_omi_transcripts(
                date=date,
                sync_all=full_history,
                write_daily_note=True,
                include_action_items=include_action_items,
                obsidian_vault=Path(os.getenv("TOTEM_VAULT_PATH", "/Users/amrit/My Obsidian Vault")),
                ledger_writer=ledger_writer,
                vault_paths=paths,
                resume_from=resume_from,
            )
        except OmiIngestCrash as e:
            console.print(f"[red]Injected crash:[/red] {e}")
            raise typer.Exit(code=1)

        errors = [IngestErrorItem(**err) for err in summary.errors]
        counts = IngestCounts(
            discovered=summary.segments_total,
            ingested=summary.segments_written,
            skipped=summary.segments_skipped,
            errored=len(errors),
        )

        record = build_manifest_record(
            source="omi",
            run_id=ledger_writer.run_id,
            run_type="full_history" if full_history else "date",
            window_start=summary.window_start.strftime("%Y-%m-%dT%H:%M:%SZ"),
            window_end=summary.window_end.strftime("%Y-%m-%dT%H:%M:%SZ"),
            counts=counts,
            last_successful_item_timestamp=summary.last_successful_item_timestamp,
            errors=errors,
            app_version=__version__,
            git_sha=git_sha,
            details={
                "conversations_count": summary.conversations_count,
                "days_processed": summary.days_processed,
                "resume_from": resume_from.strftime("%Y-%m-%dT%H:%M:%SZ") if resume_from else None,
            },
            cursor={
                "last_successful_item_timestamp": summary.last_successful_item_timestamp,
            },
        )
        _append_record(record)

        console.print(
            f"[cyan]Omi[/cyan]: segments written={summary.segments_written} "
            f"skipped={summary.segments_skipped} days={summary.days_processed}"
        )

    if all_sources or source == "chatgpt":
        if dry_run:
            console.print("[red]Error: ChatGPT local ZIP ingest does not support --dry-run[/red]")
            raise typer.Exit(code=1)

        def _progress(processed: int, total: int, conv_id: str) -> None:
            console.print(f"[dim]ChatGPT progress:[/dim] {processed}/{total} ({conv_id})")

        try:
            summary = ingest_from_downloads_with_summary(
                config=config,
                vault_paths=paths,
                ledger_writer=ledger_writer,
                downloads_dir=Path.home() / "Downloads",
                limit=50,
                progress_callback=_progress,
                routing_mode=routing_mode,
                daemon_vault_override=Path(daemon_vault) if daemon_vault else None,
                tooling_vault_override=Path(tooling_vault) if tooling_vault else None,
                reclassify=reclassify,
            )
        except LocalIngestError as e:
            console.print(f"[red]ChatGPT ingest failed:[/red] {e}")
            raise typer.Exit(code=1)

        if summary is None:
            console.print("[yellow]No valid ChatGPT export ZIP found in Downloads[/yellow]")
            return

        skipped_conversations = max(0, summary.conversations_parsed - summary.notes_written)
        counts = IngestCounts(
            discovered=summary.conversations_total,
            ingested=summary.notes_written,
            skipped=skipped_conversations,
            errored=0,
        )

        record = build_manifest_record(
            source="chatgpt",
            run_id=ledger_writer.run_id,
            run_type="full_history" if full_history else "latest",
            window_start=None,
            window_end=None,
            counts=counts,
            last_successful_item_timestamp=summary.last_successful_item_timestamp,
            errors=[],
            app_version=__version__,
            git_sha=git_sha,
            details={
                "zip_path": str(summary.zip_path),
                "conversations_total": summary.conversations_total,
                "conversations_parsed": summary.conversations_parsed,
                "notes_written": summary.notes_written,
                "downloads_dir": str(Path.home() / "Downloads"),
            },
            cursor={
                "last_successful_item_timestamp": summary.last_successful_item_timestamp,
            },
        )
        _append_record(record)

        console.print(
            f"[cyan]ChatGPT[/cyan]: notes written={summary.notes_written} "
            f"from zip={summary.zip_path.name}"
        )


@app.command("ingest-report")
def ingest_report():
    """Print a summary of the ingestion manifest."""
    vault_path = get_global_vault_path()
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    records = load_manifest_records(paths)
    if not records:
        console.print("[yellow]No manifest records found.[/yellow]")
        console.print(f"[dim]Expected: {paths.ingest_manifest_file}[/dim]")
        return

    latest = latest_record_by_source(records)
    totals = totals_by_source(records)

    table = Table(title="Ingestion Manifest Summary")
    table.add_column("Source", style="cyan", no_wrap=True)
    table.add_column("Last Run (UTC)", style="magenta")
    table.add_column("Latest Counts", style="yellow")
    table.add_column("Last Success", style="green")
    table.add_column("Total Ingested", style="dim")

    for source in ["omi", "chatgpt"]:
        record = latest.get(source)
        if not record:
            continue
        counts = record.counts
        totals_counts = totals.get(source, IngestCounts())
        table.add_row(
            source,
            record.created_at,
            f"d={counts.discovered} i={counts.ingested} s={counts.skipped} e={counts.errored}",
            record.last_successful_item_timestamp or "-",
            str(totals_counts.ingested),
        )

    console.print(table)

omi_app = typer.Typer(help="Omi transcript commands")
app.add_typer(omi_app, name="omi")

chatgpt_app = typer.Typer(help="ChatGPT export commands")
app.add_typer(chatgpt_app, name="chatgpt")


@omi_app.command("sync")
def omi_sync(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date to sync (YYYY-MM-DD, default: today)",
    ),
    sync_all: bool = typer.Option(
        False,
        "--all",
        "-a",
        help="Sync entire conversation history",
    ),
    write_daily_note: bool = typer.Option(
        True,
        "--write-daily-note/--no-write-daily-note",
        help="Write summary block to Obsidian daily note (default: True)",
    ),
    include_action_items: bool = typer.Option(
        False,
        "--include-action-items/--no-include-action-items",
        help="Include action items in daily note block (default: False)",
    ),
):
    """Sync Omi transcripts to Obsidian vault.

    By default, it syncs transcripts for today. Use --date for a
    specific day, or --all to sync your entire history.

    Fetches conversations from Omi API and writes transcripts to:
    $TOTEM_VAULT_PATH/Omi Transcripts/YYYY/MM/YYYY-MM-DD.md

    Idempotent: running multiple times will not create duplicates.
    """
    # Try to load OMI_API_KEY from .env if not in environment
    if not os.environ.get("OMI_API_KEY") and Path(".env").exists():
        for line in Path(".env").read_text().splitlines():
            if "=" in line and not line.startswith("#"):
                k, v = line.split("=", 1)
                if k.strip() == "OMI_API_KEY":
                    os.environ["OMI_API_KEY"] = v.strip().strip('"').strip("'")
                    break

    obsidian_vault_str = os.getenv("TOTEM_VAULT_PATH", "/Users/amrit/My Obsidian Vault")
    obsidian_vault = Path(obsidian_vault_str)

    vault_path = get_global_vault_path()
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Totem vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        summary = sync_omi_transcripts(
            date=date,
            sync_all=sync_all,
            write_daily_note=write_daily_note,
            include_action_items=include_action_items,
            obsidian_vault=obsidian_vault,
            ledger_writer=ledger_writer,
            vault_paths=paths,
        )

        from . import __version__
        from .config import _find_repo_root

        git_sha = try_get_git_sha(_find_repo_root(Path.cwd()))
        errors = [IngestErrorItem(**err) for err in summary.errors]
        counts = IngestCounts(
            discovered=summary.segments_total,
            ingested=summary.segments_written,
            skipped=summary.segments_skipped,
            errored=len(errors),
        )

        record = build_manifest_record(
            source="omi",
            run_id=ledger_writer.run_id,
            run_type="full_history" if sync_all else "date",
            window_start=summary.window_start.strftime("%Y-%m-%dT%H:%M:%SZ"),
            window_end=summary.window_end.strftime("%Y-%m-%dT%H:%M:%SZ"),
            counts=counts,
            last_successful_item_timestamp=summary.last_successful_item_timestamp,
            errors=errors,
            app_version=__version__,
            git_sha=git_sha,
            details={
                "conversations_count": summary.conversations_count,
                "days_processed": summary.days_processed,
            },
            cursor={
                "last_successful_item_timestamp": summary.last_successful_item_timestamp,
            },
        )
        append_manifest_record(paths, record)

        console.print()
        console.print("[bold green]Sync complete![/bold green]")
        console.print(f"  Days processed: {summary.days_processed}")
        console.print(f"  Total conversations found: {summary.conversations_count}")
        console.print(f"  New segments written: {summary.segments_written}")
        console.print(f"  Existing segments skipped: {summary.segments_skipped}")
        console.print(f"  Manifest: {paths.ingest_manifest_file}")

    except Exception as e:
        console.print(f"[red]Error during sync: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("ingest-from-zip")
def chatgpt_ingest_from_zip(
    zip_path: str = typer.Argument(..., help="Path to a local ChatGPT export ZIP file"),
    daemon_vault: Optional[str] = typer.Option(
        None,
        "--daemon-vault",
        help="Override daemon Obsidian vault path",
    ),
    tooling_vault: Optional[str] = typer.Option(
        None,
        "--tooling-vault",
        help="Override tooling Obsidian vault path",
    ),
    routing_mode: str = typer.Option(
        "heuristic",
        "--routing-mode",
        help="Routing mode {heuristic,force-daemon,force-tooling}",
    ),
    reclassify: bool = typer.Option(
        False,
        "--reclassify",
        help="Recompute routing even if stored in state",
    ),
):
    """Ingest a local ChatGPT export ZIP file."""
    # Get global vault path
    vault_path = get_global_vault_path()

    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        summary = ingest_from_zip_with_summary(
            config=config,
            vault_paths=paths,
            ledger_writer=ledger_writer,
            zip_path=Path(zip_path),
            routing_mode=routing_mode,
            daemon_vault_override=Path(daemon_vault) if daemon_vault else None,
            tooling_vault_override=Path(tooling_vault) if tooling_vault else None,
            reclassify=reclassify,
        )

        from . import __version__
        from .config import _find_repo_root

        git_sha = try_get_git_sha(_find_repo_root(Path.cwd()))
        skipped_conversations = max(0, summary.conversations_parsed - summary.notes_written)
        counts = IngestCounts(
            discovered=summary.conversations_total,
            ingested=summary.notes_written,
            skipped=skipped_conversations,
            errored=0,
        )

        record = build_manifest_record(
            source="chatgpt",
            run_id=ledger_writer.run_id,
            run_type="local_zip",
            window_start=None,
            window_end=None,
            counts=counts,
            last_successful_item_timestamp=summary.last_successful_item_timestamp,
            errors=[],
            app_version=__version__,
            git_sha=git_sha,
            details={
                "zip_path": str(summary.zip_path),
                "conversations_total": summary.conversations_total,
                "conversations_parsed": summary.conversations_parsed,
                "notes_written": summary.notes_written,
            },
            cursor={
                "last_successful_item_timestamp": summary.last_successful_item_timestamp,
            },
        )
        append_manifest_record(paths, record)

        console.print("[green]ChatGPT export ingestion completed successfully![/green]")
        console.print(f"[dim]Manifest: {paths.ingest_manifest_file}[/dim]")

    except Exception as e:
        console.print(f"[red]Error during ingestion: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("ingest-from-downloads")
def chatgpt_ingest_from_downloads(
    downloads_dir: str = typer.Option(
        str(Path.home() / "Downloads"),
        "--downloads-dir",
        help="Directory to scan for ChatGPT export ZIPs (default: ~/Downloads)",
    ),
    limit: int = typer.Option(
        50,
        "--limit",
        help="Maximum number of recent ZIP files to scan",
    ),
    daemon_vault: Optional[str] = typer.Option(
        None,
        "--daemon-vault",
        help="Override daemon Obsidian vault path",
    ),
    tooling_vault: Optional[str] = typer.Option(
        None,
        "--tooling-vault",
        help="Override tooling Obsidian vault path",
    ),
    routing_mode: str = typer.Option(
        "heuristic",
        "--routing-mode",
        help="Routing mode {heuristic,force-daemon,force-tooling}",
    ),
    reclassify: bool = typer.Option(
        False,
        "--reclassify",
        help="Recompute routing even if stored in state",
    ),
):
    """Find the newest ChatGPT export ZIP in downloads and ingest it."""
    # Get global vault path
    vault_path = get_global_vault_path()

    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        def _progress(processed: int, total: int, conv_id: str) -> None:
            console.print(f"[dim]ChatGPT progress:[/dim] {processed}/{total} ({conv_id})")

        summary = ingest_from_downloads_with_summary(
            config=config,
            vault_paths=paths,
            ledger_writer=ledger_writer,
            downloads_dir=Path(downloads_dir),
            limit=limit,
            progress_callback=_progress,
            routing_mode=routing_mode,
            daemon_vault_override=Path(daemon_vault) if daemon_vault else None,
            tooling_vault_override=Path(tooling_vault) if tooling_vault else None,
            reclassify=reclassify,
        )

        if not summary:
            console.print(
                f"[yellow]No valid ChatGPT export ZIP found in {downloads_dir}[/yellow]"
            )
            return

        from . import __version__
        from .config import _find_repo_root

        git_sha = try_get_git_sha(_find_repo_root(Path.cwd()))
        skipped_conversations = max(0, summary.conversations_parsed - summary.notes_written)
        counts = IngestCounts(
            discovered=summary.conversations_total,
            ingested=summary.notes_written,
            skipped=skipped_conversations,
            errored=0,
        )

        record = build_manifest_record(
            source="chatgpt",
            run_id=ledger_writer.run_id,
            run_type="local_downloads",
            window_start=None,
            window_end=None,
            counts=counts,
            last_successful_item_timestamp=summary.last_successful_item_timestamp,
            errors=[],
            app_version=__version__,
            git_sha=git_sha,
            details={
                "zip_path": str(summary.zip_path),
                "conversations_total": summary.conversations_total,
                "conversations_parsed": summary.conversations_parsed,
                "notes_written": summary.notes_written,
                "downloads_dir": str(downloads_dir),
            },
            cursor={
                "last_successful_item_timestamp": summary.last_successful_item_timestamp,
            },
        )
        append_manifest_record(paths, record)

        console.print("[green]ChatGPT export ingestion completed successfully![/green]")
        console.print(f"[dim]Manifest: {paths.ingest_manifest_file}[/dim]")

    except Exception as e:
        console.print(f"[red]Error during ingestion: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("backfill-metadata")
def chatgpt_backfill_metadata(
    limit: int = typer.Option(
        None,
        "--limit",
        help="Maximum number of conversation notes to process (default: config backfill_limit)",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview mode: show what would be done without writing files",
    ),
):
    """Backfill ChatGPT conversation metadata in Obsidian notes."""
    vault_path = get_global_vault_path()
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    obsidian_chatgpt_dir = paths.root / config.chatgpt_export.obsidian_chatgpt_dir

    try:
        from .chatgpt.metadata import backfill_conversation_metadata

        console.print("[cyan]Backfilling ChatGPT metadata...[/cyan]")
        with Progress(
            TextColumn("{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=console,
        ) as progress:
            task_id = progress.add_task("Backfill", total=1)

            def _on_progress(processed: int, total: int, _status: str) -> None:
                if progress.tasks[0].total != total:
                    progress.update(task_id, total=total)
                progress.update(task_id, completed=processed)

            results = backfill_conversation_metadata(
                obsidian_chatgpt_dir=obsidian_chatgpt_dir,
                summary_config=config.chatgpt_export.summary,
                ledger_writer=ledger_writer,
                limit=limit,
                dry_run=dry_run,
                progress_callback=_on_progress,
            )

        console.print(
            f"[green]Done[/green] processed={results['processed']} generated={results['generated']} "
            f"skipped={results['skipped']} failed={results['failed']}"
        )

    except Exception as e:
        console.print(f"[red]Error during metadata backfill: {e}[/red]")
        raise typer.Exit(code=1)


ledger_app = typer.Typer(help="Ledger commands")
app.add_typer(ledger_app, name="ledger")


@ledger_app.command("tail")
def ledger_tail(
    n: int = typer.Option(
        20,
        "--n",
        help="Number of recent events to display",
    ),
    full: bool = typer.Option(
        False,
        "--full",
        help="Show full payloads with JSON pretty-print",
    ),
):
    """Display the last N events from the ledger.

    Shows recent ledger events with timestamps, types, and payloads.
    Skips malformed lines with warnings.
    Use --full to see complete payloads with JSON formatting.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Read ledger tail
    events = read_ledger_tail(paths.ledger_file, n=n)

    if not events:
        console.print("[dim]No events in ledger[/dim]")
        return

    if full:
        # Full mode: show each event with pretty-printed JSON
        console.print(f"[bold]Last {len(events)} Ledger Event(s)[/bold]\n")
        for i, event in enumerate(events, 1):
            console.print(f"[cyan]Event {i}/{len(events)}[/cyan]")
            console.print(f"  [dim]Event ID:[/dim]    {event.event_id}")
            console.print(f"  [dim]Run ID:[/dim]      {event.run_id}")
            console.print(f"  [dim]Timestamp:[/dim]   {event.ts.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            console.print(f"  [dim]Event Type:[/dim]  [magenta]{event.event_type}[/magenta]")
            console.print(f"  [dim]Capture ID:[/dim]  {event.capture_id or '-'}")
            console.print(f"  [dim]Payload:[/dim]")
            
            import json
            payload_json = json.dumps(event.payload, indent=2)
            for line in payload_json.split('\n'):
                console.print(f"    {line}")
            console.print()
    else:
        # Table mode: compact view with truncation
        table = Table(title=f"Last {len(events)} Ledger Event(s)")
        table.add_column("Timestamp (UTC)", style="cyan", no_wrap=True)
        table.add_column("Event Type", style="magenta")
        table.add_column("Capture ID", style="yellow")
        table.add_column("Payload", style="dim")

        for event in events:
            # Format timestamp with UTC indicator
            ts_str = event.ts.strftime("%Y-%m-%d %H:%M:%S")
            
            # Format capture ID
            capture_id_str = event.capture_id[:8] + "..." if event.capture_id else "-"
            
            # Format payload (truncate if too long)
            payload_str = str(event.payload)
            if len(payload_str) > 60:
                payload_str = payload_str[:57] + "..."
            
            table.add_row(ts_str, event.event_type, capture_id_str, payload_str)

        console.print(table)


@app.command()
def route(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for inbox folder (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        20,
        "--limit",
        "-l",
        help="Maximum number of captures to process",
    ),
    engine: str = typer.Option(
        "auto",
        "--engine",
        "-e",
        help="Routing engine: 'rule', 'llm', 'hybrid', or 'auto' (default: auto - hybrid if API key present, else rule)",
    ),
    llm_engine: str = typer.Option(
        "auto",
        "--llm-engine",
        help="LLM engine for llm/hybrid: 'fake', 'openai', 'anthropic', or 'auto' (default: auto)",
    ),
    no_short_circuit: bool = typer.Option(
        False,
        "--no-short-circuit",
        help="Hybrid mode: always call LLM even if rule confidence is high (for A/B testing)",
    ),
    bypass_arbiter: bool = typer.Option(
        False,
        "--bypass-arbiter",
        help="Skip IntentArbiter step",
    ),
):
    """Route captures using rule-based, LLM, hybrid routing, or IntentArbiter.
    
    Reads raw captures from 00_inbox/YYYY-MM-DD/, applies routing logic,
    and writes outputs to either routed/ or review_queue/ based on confidence.
    
    By default, IntentArbiter is run first to classify intent and route to downstream agents.
    Use --bypass-arbiter to skip this and use purely the legacy routing engines.
    
    Engine modes (legacy):
    - rule: Deterministic keyword-based heuristics only
    - llm: LLM-based classification only
    - hybrid: Rule first, LLM fallback if rule confidence < threshold
    - auto: hybrid if API key present, else rule
    
    Use --no-short-circuit to force hybrid mode to always call LLM (for A/B testing).
    """
    from .llm.router import has_llm_api_key
    
    # Validate engine option
    valid_engines = ["rule", "llm", "hybrid", "auto"]
    if engine not in valid_engines:
        console.print(f"[red]Error: Invalid engine '{engine}'. Must be one of: {', '.join(valid_engines)}[/red]")
        raise typer.Exit(code=1)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine effective engine and display info
    effective_engine = engine
    if engine == "auto":
        if has_llm_api_key():
            effective_engine = "hybrid"
            console.print("[dim]Auto-detected API key: using hybrid engine[/dim]")
        else:
            effective_engine = "rule"
            console.print("[dim]No API key found: using rule engine[/dim]")
    else:
        console.print(f"[dim]Using {engine} engine[/dim]")
    
    # Display no-short-circuit mode
    if no_short_circuit:
        if effective_engine in ("hybrid", "auto"):
            console.print("[yellow]--no-short-circuit: LLM will always be called (A/B testing mode)[/yellow]")
        else:
            console.print("[dim]Note: --no-short-circuit only affects hybrid mode[/dim]")
    
    # Check if LLM is requested but no API key
    if effective_engine in ("llm", "hybrid") and llm_engine != "fake" and not has_llm_api_key():
        if llm_engine == "auto":
            console.print("[dim]No API key found: using fake LLM router[/dim]")
        else:
            console.print(f"[red]Error: LLM engine '{llm_engine}' requested but no API key found[/red]")
            console.print("[yellow]Set OPENAI_API_KEY or ANTHROPIC_API_KEY, or use --llm-engine fake[/yellow]")
            raise typer.Exit(code=1)

    # Load vault configuration (already loaded above, but needed again for some reason?)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Find inbox date folder
    inbox_date_folder = paths.inbox_date_folder(date_str)
    
    if not inbox_date_folder.exists():
        console.print(f"[yellow]No inbox folder found for {date_str}[/yellow]")
        console.print(f"[dim]Expected: {inbox_date_folder}[/dim]")
        return

    # Find all raw capture files (exclude .meta.json files)
    all_files = []
    for file_path in inbox_date_folder.iterdir():
        if file_path.is_file() and not file_path.name.endswith(".meta.json"):
            all_files.append(file_path)
    
    if not all_files:
        console.print(f"[yellow]No capture files found in {inbox_date_folder.name}/[/yellow]")
        return

    # Sort by modification time (newest first)
    all_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    
    # Apply limit
    files_to_process = all_files[:limit]
    
    if len(all_files) > limit:
        console.print(f"[dim]Found {len(all_files)} captures, processing {limit} (use --limit to change)[/dim]\n")
    else:
        console.print(f"[dim]Found {len(files_to_process)} capture(s) to process[/dim]\n")

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Process each capture
    results = []
    routed_count = 0
    review_count = 0

    for raw_file in files_to_process:
        # Find corresponding meta file
        meta_file = raw_file.with_suffix(raw_file.suffix + ".meta.json")
        
        if not meta_file.exists():
            console.print(f"[yellow]Warning: No meta file for {raw_file.name}, skipping[/yellow]")
            continue

        try:
            # Process routing with specified engine
            output_path, was_routed = process_capture_routing(
                raw_file_path=raw_file,
                meta_file_path=meta_file,
                vault_root=paths.root,
                config=config,
                ledger_writer=ledger_writer,
                date_str=date_str,
                engine=effective_engine,
                llm_engine=llm_engine,
                no_short_circuit=no_short_circuit,
            )
            
            # Integrate IntentArbiter if not bypassed
            if not bypass_arbiter:
                try:
                    # Initialize Arbiter
                    arbiter = IntentArbiterAgent(
                        ledger_writer=ledger_writer,
                        vault_root=paths.root,
                        llm_engine="auto" if has_llm_api_key() else "fake"
                    )
                    
                    # Read text again (it was read inside process_capture_routing too, but that's fine)
                    text_content = raw_file.read_text(encoding="utf-8")
                    
                    console.print(f"[dim]Running IntentArbiter on {raw_file.name}...[/dim]")
                    arbiter.run(text_content)
                    
                except Exception as e:
                    console.print(f"[red]IntentArbiter failed for {raw_file.name}: {e}[/red]")
            
            # Read the output to get details for display
            import json
            output_data = json.loads(output_path.read_text(encoding="utf-8"))
            
            results.append({
                "capture_id": output_data["capture_id"],
                "route": output_data["route_label"],
                "confidence": output_data["confidence"],
                "was_routed": was_routed,
                "output_path": output_path.relative_to(paths.root),
            })
            
            if was_routed:
                routed_count += 1
            else:
                review_count += 1

        except Exception as e:
            console.print(f"[red]Error processing {raw_file.name}: {e}[/red]")
            continue

    # Display results table
    if results:
        table = Table(title=f"Routing Results for {date_str} (engine: {effective_engine})")
        table.add_column("Capture ID", style="cyan")
        table.add_column("Route", style="magenta")
        table.add_column("Confidence", style="yellow")
        table.add_column("Destination", style="green")

        for result in results:
            capture_id_short = result["capture_id"][:8] + "..."
            confidence_str = f"{result['confidence']:.2f}"
            destination = "routed" if result["was_routed"] else "review_queue"
            
            table.add_row(
                capture_id_short,
                result["route"],
                confidence_str,
                destination
            )

        console.print(table)
        console.print()
        console.print(f"[bold green]Summary:[/bold green] {routed_count} routed, {review_count} flagged for review")
    else:
        console.print("[yellow]No captures were processed[/yellow]")


@app.command()
def intent(
    text: str = typer.Option(
        ...,
        "--text",
        "-t",
        help="Input text to classify and route",
    ),
):
    """Classify and route a single input using IntentArbiter."""
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)
    ledger_writer = LedgerWriter(paths.ledger_file)
    
    arbiter = IntentArbiterAgent(
        ledger_writer=ledger_writer,
        vault_root=paths.root,
        llm_engine="auto"
    )
    
    console.print(f"[bold]Input:[/bold] {text}")
    
    # Run arbiter (Classify -> Log -> Route -> Agent.run)
    arbiter.run(text)


@app.command()
def distill(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for processing (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        20,
        "--limit",
        "-l",
        help="Maximum number of routed items to process",
    ),
    engine: str = typer.Option(
        "auto",
        "--engine",
        "-e",
        help="LLM engine: 'fake', 'real', 'openai', 'anthropic', or 'auto' (default: auto)",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview distillation without writing canon files",
    ),
):
    """Distill routed captures using LLM and apply canon writes.

    Reads from 10_derived/routed/YYYY-MM-DD/, processes through LLM distillation,
    and writes results to distill/, daily notes, todo, and entities.
    All writes are append-only with undo support via 'totem undo'.

    Use --dry-run to preview what would be written without applying changes.
    """
    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Get LLM client
    try:
        llm_client = get_llm_client(engine)
        console.print(f"[dim]Using LLM engine: {llm_client.engine_name}[/dim]")
        if llm_client.provider_model:
            console.print(f"[dim]Provider/model: {llm_client.provider_model}[/dim]")
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)

    # Load routed items
    routed_items = load_routed_items(paths, date_str, limit=limit)
    
    if not routed_items:
        console.print(f"[yellow]No routed items found for {date_str}[/yellow]")
        console.print(f"[dim]Expected: {paths.routed_date_folder(date_str)}[/dim]")
        return

    if dry_run:
        console.print(f"[yellow]DRY-RUN MODE[/yellow] — Found {len(routed_items)} routed item(s) to preview\n")
    else:
        console.print(f"[dim]Found {len(routed_items)} routed item(s) to process[/dim]\n")

    # Initialize ledger writer (only used in non-dry-run mode)
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Process each routed item
    results = []
    for item in routed_items:
        capture_id = item.get("capture_id", "unknown")
        capture_id_short = capture_id[:8] + "..."
        
        try:
            console.print(f"[cyan]Processing:[/cyan] {capture_id_short}")
            
            if dry_run:
                # Dry-run mode: generate but don't write canon files
                distill_result, would_apply, distill_path = process_distillation_dry_run(
                    routed_item=item,
                    llm_client=llm_client,
                    vault_paths=paths,
                    date_str=date_str,
                )
                
                results.append({
                    "capture_id": capture_id,
                    "confidence": distill_result.confidence,
                    "summary": distill_result.summary[:50] + "..." if len(distill_result.summary) > 50 else distill_result.summary,
                    "tasks_count": len(distill_result.tasks),
                    "entities_count": len(distill_result.entities),
                    "would_modify": len(would_apply),
                    "distill_path": distill_path,
                    "would_apply": would_apply,
                })
                
                console.print(f"  [green]+[/green] Distilled (confidence: {distill_result.confidence:.2f})")
                console.print(f"    Distill artifact: [dim]{distill_path}[/dim]")
                console.print(f"    [yellow]Would write to {len(would_apply)} file(s):[/yellow]")
                for af in would_apply:
                    console.print(f"      - {af.path}")
            else:
                # Normal mode: write canon files
                distill_result, write_record = process_distillation(
                    routed_item=item,
                    llm_client=llm_client,
                    vault_paths=paths,
                    ledger_writer=ledger_writer,
                    date_str=date_str,
                )
                
                results.append({
                    "capture_id": capture_id,
                    "write_id": write_record.write_id,
                    "confidence": distill_result.confidence,
                    "summary": distill_result.summary[:50] + "..." if len(distill_result.summary) > 50 else distill_result.summary,
                    "tasks_count": len(distill_result.tasks),
                    "entities_count": len(distill_result.entities),
                    "modified_files": len(write_record.applied_files),
                })
                
                console.print(f"  [green]+[/green] Distilled (confidence: {distill_result.confidence:.2f})")
                console.print(f"    Write ID: [yellow]{write_record.write_id[:8]}...[/yellow]")
            
        except Exception as e:
            console.print(f"  [red]x Error: {e}[/red]")
            continue

    # Summary
    console.print()
    if results:
        if dry_run:
            table = Table(title=f"Dry-Run Preview — {date_str}")
            table.add_column("Capture", style="cyan")
            table.add_column("Conf", style="green")
            table.add_column("Tasks", style="magenta")
            table.add_column("Entities", style="blue")
            table.add_column("Would Write", style="yellow")
            
            for r in results:
                table.add_row(
                    r["capture_id"][:8] + "...",
                    f"{r['confidence']:.2f}",
                    str(r["tasks_count"]),
                    str(r["entities_count"]),
                    str(r["would_modify"]),
                )
            
            console.print(table)
            console.print()
            console.print(f"[bold yellow]DRY-RUN Summary:[/bold yellow] {len(results)} item(s) would be distilled")
            console.print("[dim]Distill artifacts were created. Canon files were NOT modified.[/dim]")
            console.print("[dim]Run without --dry-run to apply changes.[/dim]")
        else:
            table = Table(title=f"Distillation Results — {date_str}")
            table.add_column("Capture", style="cyan")
            table.add_column("Write ID", style="yellow")
            table.add_column("Conf", style="green")
            table.add_column("Tasks", style="magenta")
            table.add_column("Entities", style="blue")
            
            for r in results:
                table.add_row(
                    r["capture_id"][:8] + "...",
                    r["write_id"][:8] + "...",
                    f"{r['confidence']:.2f}",
                    str(r["tasks_count"]),
                    str(r["entities_count"]),
                )
            
            console.print(table)
            console.print()
            console.print(f"[bold green]Summary:[/bold green] {len(results)} item(s) distilled")
            console.print("[dim]Use 'totem undo --write-id <ID>' to reverse any write[/dim]")
    else:
        console.print("[yellow]No items were successfully distilled[/yellow]")


@app.command()
def undo(
    write_id: str = typer.Option(
        ...,
        "--write-id",
        "-w",
        help="Write ID (UUID) to undo",
    ),
):
    """Undo a canon write by removing inserted blocks.

    Reverses the append-only writes from a distillation operation.
    The write ID can be found in the distillation output or ledger.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        console.print(f"[cyan]Undoing write:[/cyan] {write_id}")
        
        modified_files = undo_canon_write(
            write_id=write_id,
            vault_paths=paths,
            ledger_writer=ledger_writer,
        )
        
        console.print()
        console.print("[bold green]Undo successful![/bold green]")
        
        if modified_files:
            console.print("[dim]Modified files:[/dim]")
            for path in modified_files:
                console.print(f"  - {path}")
        else:
            console.print("[yellow]No files were modified[/yellow]")
        
        console.print()
        console.print("[dim]Note: entities.json changes require manual review[/dim]")
        
    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        console.print("[dim]Check the write ID is correct (from distill output or ledger)[/dim]")
        raise typer.Exit(code=1)
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)
    except Exception as e:
        console.print(f"[red]Unexpected error: {e}[/red]")
        raise typer.Exit(code=1)


@app.command()
def review(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for proposals (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        None,
        "--limit",
        "-l",
        help="Maximum number of proposals to review",
    ),
    queue_path: str = typer.Option(
        None,
        "--queue",
        "-q",
        help="Override path to review queue file",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview mode: show proposals without writing artifacts",
    ),
):
    """Interactive review loop for proposed artifacts.

    Single-keystroke review interface:
      [A]pprove  - Write proposal to canon
      [V]eto     - Discard proposal (logs learning event)
      [C]orrect  - Override with corrected artifact
      [D]efer    - Keep in queue for later
      [Q]uit     - Exit review session

    Philosophy: You never file, only judge. No silent writes.
    """
    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine queue path
    if queue_path:
        review_queue_file = Path(queue_path)
    else:
        review_queue_file = paths.review_queue_file

    # Show mode
    if dry_run:
        console.print("[yellow]DRY-RUN MODE[/yellow] — No artifacts will be written\n")
    
    console.print(f"[dim]Date: {date_str}[/dim]")
    console.print(f"[dim]Queue: {review_queue_file}[/dim]\n")

    # Load or create proposals
    console.print("[dim]Loading proposals...[/dim]")
    proposals = load_or_create_proposals(paths, date_str)
    
    if not proposals:
        console.print("[yellow]No proposals to review.[/yellow]")
        console.print(f"[dim]Run 'totem distill --date {date_str}' first to generate proposals.[/dim]")
        return

    console.print(f"[green]Found {len(proposals)} proposal(s) to review.[/green]\n")

    # Initialize components
    review_queue = ReviewQueue(review_queue_file)
    learning_logger = LearningEventLogger(paths.review_events_file)
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Create and run session
    session = ReviewSession(
        review_queue=review_queue,
        learning_logger=learning_logger,
        vault_paths=paths,
        ledger_writer=ledger_writer,
        dry_run=dry_run,
        output_fn=print,  # Use print for terminal output
    )

    try:
        summary = session.run(limit=limit)
    except KeyboardInterrupt:
        console.print("\n[yellow]Review session interrupted.[/yellow]")
        summary = session._get_summary()

    # Display summary
    console.print("\n" + "=" * 40)
    console.print("[bold]Review Session Summary[/bold]")
    console.print("=" * 40)
    console.print(f"  Approved:  {summary['approved']}")
    console.print(f"  Vetoed:    {summary['vetoed']}")
    console.print(f"  Corrected: {summary['corrected']}")
    console.print(f"  Deferred:  {summary['deferred']}")
    console.print(f"  [bold]Total:     {summary['total']}[/bold]")
    
    if dry_run:
        console.print("\n[yellow]DRY-RUN: No changes were made.[/yellow]")
    else:
        console.print(f"\n[dim]Learning events logged to: {paths.review_events_file}[/dim]")


@app.command()
def version():
    """Show Totem OS version."""
    from . import __version__
    console.print(f"Totem OS v{__version__}")


@daemon_app.command("index")
def daemon_index(
    vault: Optional[str] = typer.Option(
        None,
        "--vault",
        help="Path to daemon Obsidian vault root (overrides .totem/config.toml [obsidian.vaults].daemon_path)",
    ),
    full: bool = typer.Option(
        False,
        "--full",
        help="Drop and recreate schema, then reindex all markdown files",
    ),
    db_path: Optional[str] = typer.Option(
        None,
        "--db-path",
        help="SQLite DB path (relative to daemon vault unless absolute; overrides [daemon].daemon_index_sqlite)",
    ),
):
    """Index the daemon Obsidian vault (index-only)."""
    from .daemon_index.config import load_daemon_index_config
    from .daemon_index.indexer import index_daemon_vault

    try:
        cfg = load_daemon_index_config(cli_vault=vault, cli_db_path=db_path)
    except (FileNotFoundError, ValueError) as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(code=1)

    summary = index_daemon_vault(cfg, full=full)
    console.print(f"scanned={summary.scanned} updated={summary.updated} unchanged={summary.unchanged} deleted={summary.deleted}")

@daemon_app.command("embed")
def daemon_embed(
    vault: Optional[str] = typer.Option(
        None,
        "--vault",
        help="Path to daemon Obsidian vault root (overrides .totem/config.toml [obsidian.vaults].daemon_path)",
    ),
    full: bool = typer.Option(
        False,
        "--full",
        help="Recompute chunks for all files (non-destructive to chunk_embeddings table)",
    ),
    limit: Optional[int] = typer.Option(
        None,
        "--limit",
        help="Maximum number of missing chunk embeddings to compute this run",
    ),
    db_path: Optional[str] = typer.Option(
        None,
        "--db-path",
        help="SQLite DB path (relative to daemon vault unless absolute; overrides [daemon].daemon_index_sqlite)",
    ),
):
    """Compute chunks + embeddings for the daemon vault (no retrieval yet)."""
    from .daemon_embed.config import load_daemon_embed_config
    from .daemon_embed.orchestrator import embed_daemon_vault

    try:
        cfg = load_daemon_embed_config(cli_vault=vault, cli_db_path=db_path)
    except (FileNotFoundError, ValueError, RuntimeError) as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(code=1)

    summary = embed_daemon_vault(cfg, full=full, limit=limit)
    console.print(
        "files_considered={files_considered} files_rechunked={files_rechunked} "
        "chunks_upserted={chunks_upserted} chunks_embedded={chunks_embedded} "
        "files_embedded={files_embedded} dangling_embeddings_deleted={dangling_embeddings_deleted}".format(
            files_considered=summary.files_considered,
            files_rechunked=summary.files_rechunked,
            chunks_upserted=summary.chunks_upserted,
            chunks_embedded=summary.chunks_embedded,
            files_embedded=summary.files_embedded,
            dangling_embeddings_deleted=summary.dangling_embeddings_deleted,
        )
    )

@daemon_app.command("fts-rebuild")
def daemon_fts_rebuild(
    vault: Optional[str] = typer.Option(
        None,
        "--vault",
        help="Path to daemon Obsidian vault root (overrides .totem/config.toml [obsidian.vaults].daemon_path)",
    ),
    full: bool = typer.Option(
        False,
        "--full",
        help="Rebuild FTS index from scratch",
    ),
    db_path: Optional[str] = typer.Option(
        None,
        "--db-path",
        help="SQLite DB path (relative to daemon vault unless absolute; overrides [daemon].daemon_index_sqlite)",
    ),
):
    """Rebuild/update the chunk FTS5 index from existing chunks."""
    from .daemon_search.config import load_daemon_search_config
    from .daemon_search.db import connect
    from .daemon_search.fts import rebuild_chunk_fts

    try:
        cfg = load_daemon_search_config(cli_vault=vault, cli_db_path=db_path)
    except (FileNotFoundError, ValueError, RuntimeError) as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(code=1)

    conn = connect(cfg.db_path)
    try:
        stats = rebuild_chunk_fts(conn, vault_root=cfg.vault_root, full=full)
    finally:
        conn.close()

    console.print(f"inserted={stats['inserted']} updated={stats['updated']} skipped={stats['skipped']}")


@daemon_app.command("search")
def daemon_search(
    query: str = typer.Argument(..., help="Search query"),
    top_k: Optional[int] = typer.Option(None, "--top-k", help="Number of primary results to return"),
    tag: Optional[list[str]] = typer.Option(None, "--tag", help="Filter by tag (repeatable)"),
    tag_or: bool = typer.Option(False, "--tag-or", help="If set, tags are ORed (default AND)"),
    date_from: Optional[str] = typer.Option(None, "--date-from", help="Filter: effective date >= YYYY-MM-DD"),
    date_to: Optional[str] = typer.Option(None, "--date-to", help="Filter: effective date <= YYYY-MM-DD"),
    prefer_recent: bool = typer.Option(False, "--prefer-recent", help="Apply deterministic recency boost"),
    expand_links: Optional[int] = typer.Option(
        None,
        "--expand-links",
        help="0 disables; >0 appends up to N (capped) 1-hop neighbors after primary hits",
    ),
    vault: Optional[str] = typer.Option(
        None,
        "--vault",
        help="Path to daemon Obsidian vault root (overrides .totem/config.toml [obsidian.vaults].daemon_path)",
    ),
    db_path: Optional[str] = typer.Option(
        None,
        "--db-path",
        help="SQLite DB path (relative to daemon vault unless absolute; overrides [daemon].daemon_index_sqlite)",
    ),
):
    """Hybrid daemon search (FTS5 + vectors), returns bounded excerpts only."""
    from .daemon_search.config import load_daemon_search_config
    from .daemon_search.engine import search_daemon
    from .daemon_search.models import SearchFilters

    try:
        cfg = load_daemon_search_config(cli_vault=vault, cli_db_path=db_path)
    except (FileNotFoundError, ValueError, RuntimeError) as e:
        console.print(f"[red]Error:[/red] {e}")
        raise typer.Exit(code=1)

    k = int(top_k) if top_k is not None else int(cfg.top_k_default)
    expand = int(expand_links) if expand_links is not None else int(cfg.expand_links_default)
    tags = tag or []

    hits = search_daemon(
        cfg,
        query=query,
        top_k=k,
        prefer_recent=prefer_recent,
        filters=SearchFilters(tags=tags, tag_or=tag_or, date_from=date_from, date_to=date_to),
        expand_links=expand,
    )

    table = Table(title="Daemon Search Results", show_lines=False)
    table.add_column("Score", justify="right")
    table.add_column("Date", justify="left")
    table.add_column("Path", overflow="fold")
    table.add_column("Heading", overflow="fold")
    table.add_column("Excerpt", overflow="fold")
    table.add_column("Expanded", justify="center")

    for h in hits:
        table.add_row(
            f"{h.score:.4f}" if not h.expanded_context else "",
            h.effective_date,
            h.rel_path,
            h.heading_path,
            h.excerpt,
            "1" if h.expanded_context else "0",
        )

    console.print(table)


def main():
    """Entry point for the CLI."""
    # Handle --version before Typer processing
    if len(sys.argv) == 2 and sys.argv[1] in ("--version", "-v"):
        from . import __version__
        console.print(f"Totem OS v{__version__}")
        sys.exit(0)

    app()


if __name__ == "__main__":
    main()
