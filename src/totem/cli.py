"""Typer-based CLI for Totem OS."""

import logging
import sys
from datetime import datetime, timezone
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table

from .capture import ingest_file_capture, ingest_text_capture
from .config import TotemConfig
from .distill import (
    load_routed_items,
    process_distillation,
    process_distillation_dry_run,
    undo_canon_write,
)
from .ledger import LedgerWriter, read_ledger_tail
from .llm import get_llm_client
from .paths import VaultPaths
from .review import (
    KeyInputSource,
    LearningEventLogger,
    ReviewQueue,
    ReviewSession,
    load_or_create_proposals,
)
from .route import process_capture_routing
from .agents.intent_arbiter import IntentArbiterAgent
from .models.intent import IntentType

# Global vault path storage
_global_vault_path = None

def set_global_vault_path(vault_path: str = typer.Option(
    None,
    "--vault",
    "-v",
    help="Path to vault directory (default: auto-resolve from current directory or repo config)",
)):
    """Global vault path option callback."""
    global _global_vault_path
    _global_vault_path = vault_path

def get_global_vault_path():
    """Get the global vault path set by CLI option."""
    return _global_vault_path

app = typer.Typer(
    name="totem",
    help="Totem OS - Local-first personal cognitive operating system",
    add_completion=False,
    callback=set_global_vault_path,
)



console = Console()


@app.command()
def link_vault(
    vault_path: str = typer.Argument(..., help="Path to existing Totem vault directory"),
):
    """Link an existing vault to the current repository.

    Creates .totem/config.toml in the repository root with the vault path.
    This allows running Totem commands from anywhere within the repo.
    """
    from .config import _find_repo_root, _has_vault_markers

    vault_path = Path(vault_path).resolve()

    # Validate vault exists and has markers
    if not vault_path.exists():
        console.print(f"[red]Error: Vault path does not exist: {vault_path}[/red]")
        raise typer.Exit(code=1)

    if not _has_vault_markers(vault_path):
        console.print(f"[red]Error: Path exists but is not a valid Totem vault: {vault_path}[/red]")
        console.print("[yellow]Vault must contain 90_system/config.yaml[/yellow]")
        raise typer.Exit(code=1)

    # Find repo root
    repo_root = _find_repo_root(Path.cwd())

    # Create .totem directory
    totem_dir = repo_root / ".totem"
    totem_dir.mkdir(exist_ok=True)

    # Write config.toml
    config_file = totem_dir / "config.toml"
    config_content = f"""# Totem OS repository configuration
# Auto-generated by 'totem link-vault'

vault_root = "{vault_path}"
"""

    config_file.write_text(config_content, encoding="utf-8")

    console.print(f"[green]✓[/green] Linked vault: {vault_path}")
    console.print(f"[green]✓[/green] Created config: {config_file}")
    console.print(f"[dim]Repository root: {repo_root}[/dim]")
    console.print()
    console.print("[dim]You can now run Totem commands from anywhere in this repository.[/dim]")


@app.command()
def init(
    force: bool = typer.Option(
        False,
        "--force",
        "-f",
        help="Re-initialize even if vault already exists",
    ),
):
    """Initialize a new Totem OS vault with directory structure and system files.

    This command is idempotent - it will not overwrite existing data.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load configuration from environment or use defaults (create_ok mode for init)
    config = TotemConfig.from_env(cli_vault_path=vault_path, mode="create_ok")
    
    vault_root = config.vault_path
    paths = VaultPaths.from_config(config)
    
    # Check if vault already exists
    if vault_root.exists() and not force:
        # Check if it looks like a vault (has system directory)
        if paths.system.exists():
            console.print(f"[yellow]Vault already exists at:[/yellow] {vault_root}")
            console.print("[yellow]Running in idempotent mode - will only create missing items[/yellow]")
        else:
            console.print(f"[yellow]Directory exists but is not a vault:[/yellow] {vault_root}")
            console.print("[yellow]Initializing vault structure...[/yellow]")
    else:
        console.print(f"[green]Initializing new Totem OS vault at:[/green] {vault_root}")
    
    # Create all directories (idempotent - won't fail if exists)
    directories_created = []
    for directory in paths.get_all_directories():
        if not directory.exists():
            directory.mkdir(parents=True, exist_ok=True)
            directories_created.append(directory)
    
    if directories_created:
        console.print(f"[green]+[/green] Created {len(directories_created)} directories")
    else:
        console.print("[dim]All directories already exist[/dim]")
    
    # Create config.yaml if it doesn't exist
    if not paths.config_file.exists():
        paths.config_file.write_text(config.to_yaml_str())
        console.print(f"[green]+[/green] Created config: {paths.config_file}")
    else:
        console.print(f"[dim]Config already exists: {paths.config_file}[/dim]")
    
    # Create empty ledger.jsonl if it doesn't exist
    if not paths.ledger_file.exists():
        paths.ledger_file.touch()
        console.print(f"[green]+[/green] Created ledger: {paths.ledger_file}")
    else:
        console.print(f"[dim]Ledger already exists: {paths.ledger_file}[/dim]")
    
    # Create empty entities.json if it doesn't exist
    if not paths.entities_file.exists():
        paths.entities_file.write_text("[]")
        console.print(f"[green]+[/green] Created entities: {paths.entities_file}")
    else:
        console.print(f"[dim]Entities file already exists: {paths.entities_file}[/dim]")
    
    # Create empty todo.md if it doesn't exist
    if not paths.todo_file.exists():
        todo_template = """# Totem OS - Next Actions

<!--
Max 3 actions at a time.
Format: - [ ] action description
-->

"""
        paths.todo_file.write_text(todo_template)
        console.print(f"[green]+[/green] Created todo: {paths.todo_file}")
    else:
        console.print(f"[dim]Todo file already exists: {paths.todo_file}[/dim]")
    
    # Create empty principles.md if it doesn't exist
    if not paths.principles_file.exists():
        principles_template = """# Personal Principles

<!--
This file captures your evolving principles and values.
Updated by distillation when decisions reveal patterns.
-->

"""
        paths.principles_file.write_text(principles_template)
        console.print(f"[green]+[/green] Created principles: {paths.principles_file}")
    else:
        console.print(f"[dim]Principles file already exists: {paths.principles_file}[/dim]")
    
    console.print()
    console.print("[bold green]Vault initialization complete![/bold green]")
    console.print(f"[dim]Vault location:[/dim] {vault_root.absolute()}")


@app.command()
def capture(
    text: str = typer.Option(
        None,
        "--text",
        "-t",
        help="Capture text content directly",
    ),
    file: str = typer.Option(
        None,
        "--file",
        "-f",
        help="Capture file by copying into vault inbox",
    ),
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for inbox folder (YYYY-MM-DD, default: today)",
    ),
):
    """Capture text or file into the vault inbox.

    Creates raw file + .meta.json sidecar in 00_inbox/YYYY-MM-DD/.
    Appends CAPTURE_INGESTED event to ledger.jsonl.
    """
    # Validate: exactly one of --text or --file must be provided
    if not text and not file:
        console.print("[red]Error: Must provide either --text or --file[/red]")
        raise typer.Exit(code=1)

    if text and file:
        console.print("[red]Error: Cannot provide both --text and --file[/red]")
        raise typer.Exit(code=1)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        if text:
            # Ingest text capture
            raw_path, meta_path, capture_id = ingest_text_capture(
                vault_inbox=paths.inbox,
                text=text,
                ledger_writer=ledger_writer,
                date_str=date_str,
            )
            console.print("[green]Captured text:[/green]")
            console.print(f"  Raw:  {raw_path.relative_to(paths.root)}")
            console.print(f"  Meta: {meta_path.relative_to(paths.root)}")
            console.print(f"  ID:   {capture_id}")
        
        elif file:
            # Ingest file capture
            source_path = Path(file)
            raw_path, meta_path, capture_id = ingest_file_capture(
                vault_inbox=paths.inbox,
                source_file_path=source_path,
                ledger_writer=ledger_writer,
                date_str=date_str,
            )
            console.print("[green]Captured file:[/green]")
            console.print(f"  Raw:  {raw_path.relative_to(paths.root)}")
            console.print(f"  Meta: {meta_path.relative_to(paths.root)}")
            console.print(f"  ID:   {capture_id}")

    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)
    except Exception as e:
        console.print(f"[red]Error during capture: {e}[/red]")
        raise typer.Exit(code=1)


omi_app = typer.Typer(help="Omi transcript commands")
app.add_typer(omi_app, name="omi")

chatgpt_app = typer.Typer(help="ChatGPT export commands")
app.add_typer(chatgpt_app, name="chatgpt")


@omi_app.command("sync")
def omi_sync(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date to sync (YYYY-MM-DD, default: today)",
    ),
    sync_all: bool = typer.Option(
        False,
        "--all",
        "-a",
        help="Sync entire conversation history",
    ),
    write_daily_note: bool = typer.Option(
        True,
        "--write-daily-note/--no-write-daily-note",
        help="Write summary block to Obsidian daily note (default: True)",
    ),
):
    """Sync Omi transcripts to Obsidian vault.

    By default, it syncs transcripts for today. Use --date for a
    specific day, or --all to sync your entire history.

    Fetches conversations from Omi API and writes transcripts to:
    $TOTEM_VAULT_PATH/Omi Transcripts/YYYY/MM/YYYY-MM-DD.md

    Idempotent: running multiple times will not create duplicates.
    """
    import os
    from datetime import datetime, timezone
    from pathlib import Path
    from collections import defaultdict

    from .omi.client import OmiClient
    from .omi.daily_note import write_daily_note_omi_block
    from .omi.writer import write_transcripts_to_vault
    from .omi.trace import write_sync_trace

    # Try to load OMI_API_KEY from .env if not in environment
    if not os.environ.get("OMI_API_KEY") and Path(".env").exists():
        for line in Path(".env").read_text().splitlines():
            if "=" in line and not line.startswith("#"):
                k, v = line.split("=", 1)
                if k.strip() == "OMI_API_KEY":
                    os.environ["OMI_API_KEY"] = v.strip().strip('"').strip("'")
                    break

    # Read Obsidian vault path from env var or use default
    obsidian_vault_str = os.getenv("TOTEM_VAULT_PATH", "/Users/amrit/My Obsidian Vault")
    obsidian_vault = Path(obsidian_vault_str)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load Totem vault configuration for ledger
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)
    
    # Check if Totem vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Totem vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)
    
    # Determine date range
    if sync_all:
        # Start from a distant past to get everything
        start_date = datetime(2020, 1, 1, 0, 0, 0)
        end_date = datetime.now()
        console.print("[cyan]Syncing all Omi history...[/cyan]")
    else:
        # Determine specific date (today if not provided)
        if date:
            date_str = date
            try:
                datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                console.print(f"[red]Error: Invalid date format '{date_str}'. Use YYYY-MM-DD[/red]")
                raise typer.Exit(code=1)
        else:
            date_str = datetime.now().strftime("%Y-%m-%d")
        
        console.print(f"[cyan]Syncing Omi transcripts for {date_str}...[/cyan]")
        year, month, day = map(int, date_str.split("-"))
        start_date = datetime(year, month, day, 0, 0, 0)
        end_date = datetime(year, month, day, 23, 59, 59)
    
    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)
    
    try:
        # Initialize Omi client
        client = OmiClient()
        
        # Track timing
        sync_start = datetime.now(timezone.utc)
        
        # Fetch conversations
        console.print("[dim]Fetching conversations from Omi API...[/dim]")
        conversations = client.fetch_conversations(start_date, end_date)
        
        if not conversations:
            label = "history" if sync_all else date_str
            console.print(f"[yellow]No conversations found for {label}[/yellow]")
            return
        
        console.print(f"[dim]Found {len(conversations)} conversation(s) total[/dim]")
        
        # Group conversations by date
        by_date = defaultdict(list)
        for conv in conversations:
            # Use started_at to determine the daily file
            d_str = conv.started_at.strftime("%Y-%m-%d")
            by_date[d_str].append(conv)
            
        # Extract metadata for trace
        conversation_ids = [conv.id for conv in conversations]
        
        # Log fetch event
        ledger_writer.append_event(
            event_type="OMI_SYNC_FETCHED",
            payload={
                "range_start": start_date.isoformat(),
                "range_end": end_date.isoformat(),
                "conversations_count": len(conversations),
                "api_endpoint": f"{client.BASE_URL}/user/conversations",
                "sync_all": sync_all
            },
        )
        
        # Process each date
        total_written = 0
        total_skipped = 0
        days_processed = 0
        
        # Sort keys so we write in chronological order
        for d_str in sorted(by_date.keys()):
            day_convs = by_date[d_str]
            console.print(f"[dim]Processing {d_str} ({len(day_convs)} convs)...[/dim]")
            
            result = write_transcripts_to_vault(
                conversations=day_convs,
                date_str=d_str,
                vault_root=obsidian_vault,
                ledger_writer=ledger_writer,
            )
            
            # Write daily note block if requested
            daily_note_result = None
            if write_daily_note:
                try:
                    daily_note_result = write_daily_note_omi_block(
                        conversations=day_convs,
                        date_str=d_str,
                        vault_root=obsidian_vault,
                        ledger_writer=ledger_writer,
                    )
                    console.print(f"[green]  + Updated daily note: {daily_note_result.daily_note_path.name}[/green]")
                except Exception as e:
                    console.print(f"[red]  ! Failed to write daily note: {e}[/red]")
            
            total_written += result.segments_written
            total_skipped += result.segments_skipped
            days_processed += 1
        
        sync_end = datetime.now(timezone.utc)
        
        # Write trace (using the target date of the run or "bulk" if all)
        trace_date_label = "history" if sync_all else date_str
        
        # If bulk sync, we'll use today's folder for the trace
        trace_folder_date = datetime.now().strftime("%Y-%m-%d")
        
        # Create a combined result for trace
        from .models.omi import OmiSyncResult
        combined_result = OmiSyncResult(
            date=trace_date_label,
            conversations_count=len(conversations),
            segments_written=total_written,
            segments_skipped=total_skipped,
            file_path=obsidian_vault # Reference path
        )
        
        write_sync_trace(
            sync_result=combined_result,
            run_id=ledger_writer.run_id,
            vault_paths=paths,
            date_str=trace_folder_date,
            start_time=sync_start,
            end_time=sync_end,
            api_endpoint=f"{client.BASE_URL}/user/conversations",
            api_params={
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "include_transcript": "true",
                "limit": 100,
                "sync_all": sync_all,
                "write_daily_note": write_daily_note
            },
            conversation_ids=conversation_ids,
            daily_note_written=write_daily_note and days_processed > 0,  # Rough approximation for trace metadata
        )
        
        # Display summary
        console.print()
        console.print("[bold green]Sync complete![/bold green]")
        console.print(f"  Days processed: {days_processed}")
        console.print(f"  Total conversations found: {len(conversations)}")
        console.print(f"  New segments written: {total_written}")
        console.print(f"  Existing segments skipped: {total_skipped}")
        
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)
    except Exception as e:
        console.print(f"[red]Error during sync: {e}[/red]")
        # stack trace for debugging if needed
        # raise
        raise typer.Exit(code=1)


@chatgpt_app.command("ingest-latest-export")
def chatgpt_ingest_latest_export(
    debug: bool = typer.Option(
        False,
        "--debug",
        help="Enable debug logging",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview mode: show what would be done without making changes",
    ),
    lookback_days: int = typer.Option(
        None,
        "--lookback-days",
        help="Override default Gmail query lookback period in days",
    ),
    gmail_query: str = typer.Option(
        None,
        "--gmail-query",
        help="Override Gmail query completely (for debugging)",
    ),
    allow_http_download: bool = typer.Option(
        False,
        "--allow-http-download",
        help="Allow HTTP download for ChatGPT export URLs (not recommended)",
    ),
):
    """Ingest the latest unprocessed ChatGPT export from Gmail.

    Downloads the most recent export email, extracts conversations, and writes
    Obsidian notes. Idempotent - safe to run repeatedly.

    Requires Gmail API credentials and proper configuration.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)

    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Set up logging
    import logging as _logging
    log_level = _logging.DEBUG if debug else _logging.INFO
    _logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        from .chatgpt.ingest import ingest_latest_export

        success = ingest_latest_export(
            config=config,
            vault_paths=paths,
            ledger_writer=ledger_writer,
            debug=debug,
            dry_run=dry_run,
            lookback_days=lookback_days,
            gmail_query_override=gmail_query,
            allow_http_download=allow_http_download,
        )

        if success:
            if dry_run:
                console.print("[green]DRY RUN completed successfully - no changes made[/green]")
            else:
                console.print("[green]ChatGPT export ingestion completed successfully![/green]")
        else:
            console.print("[yellow]No new exports to process[/yellow]")

    except Exception as e:
        console.print(f"[red]Error during ingestion: {e}[/red]")
        if debug:
            import traceback
            console.print(traceback.format_exc())
        raise typer.Exit(code=1)


@chatgpt_app.command("ingest-from-zip")
def chatgpt_ingest_from_zip(
    zip_path: str = typer.Argument(..., help="Path to a local ChatGPT export ZIP file"),
):
    """Ingest a local ChatGPT export ZIP file."""
    # Get global vault path
    vault_path = get_global_vault_path()

    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        from .chatgpt.local_ingest import ingest_from_zip

        ingest_from_zip(
            config=config,
            vault_paths=paths,
            ledger_writer=ledger_writer,
            zip_path=Path(zip_path),
        )
        console.print("[green]ChatGPT export ingestion completed successfully![/green]")

    except Exception as e:
        console.print(f"[red]Error during ingestion: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("ingest-from-downloads")
def chatgpt_ingest_from_downloads(
    downloads_dir: str = typer.Option(
        str(Path.home() / "Downloads"),
        "--downloads-dir",
        help="Directory to scan for ChatGPT export ZIPs (default: ~/Downloads)",
    ),
    limit: int = typer.Option(
        50,
        "--limit",
        help="Maximum number of recent ZIP files to scan",
    ),
):
    """Find the newest ChatGPT export ZIP in downloads and ingest it."""
    # Get global vault path
    vault_path = get_global_vault_path()

    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        from .chatgpt.local_ingest import ingest_from_downloads

        success = ingest_from_downloads(
            config=config,
            vault_paths=paths,
            ledger_writer=ledger_writer,
            downloads_dir=Path(downloads_dir),
            limit=limit,
        )

        if success:
            console.print("[green]ChatGPT export ingestion completed successfully![/green]")
        else:
            console.print(
                f"[yellow]No valid ChatGPT export ZIP found in {downloads_dir}[/yellow]"
            )

    except Exception as e:
        console.print(f"[red]Error during ingestion: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("backfill-metadata")
def chatgpt_backfill_metadata(
    limit: int = typer.Option(
        None,
        "--limit",
        help="Maximum number of conversation notes to process (default: config backfill_limit)",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview mode: show what would be done without writing files",
    ),
):
    """Backfill ChatGPT conversation metadata in Obsidian notes."""
    vault_path = get_global_vault_path()
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    ledger_writer = LedgerWriter(paths.ledger_file)

    obsidian_chatgpt_dir = paths.root / config.chatgpt_export.obsidian_chatgpt_dir

    try:
        from .chatgpt.metadata import backfill_conversation_metadata

        console.print("[cyan]Backfilling ChatGPT metadata...[/cyan]")
        with Progress(
            TextColumn("{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=console,
        ) as progress:
            task_id = progress.add_task("Backfill", total=1)

            def _on_progress(processed: int, total: int, _status: str) -> None:
                if progress.tasks[0].total != total:
                    progress.update(task_id, total=total)
                progress.update(task_id, completed=processed)

            results = backfill_conversation_metadata(
                obsidian_chatgpt_dir=obsidian_chatgpt_dir,
                summary_config=config.chatgpt_export.summary,
                ledger_writer=ledger_writer,
                limit=limit,
                dry_run=dry_run,
                progress_callback=_on_progress,
            )

        console.print(
            f"[green]Done[/green] processed={results['processed']} generated={results['generated']} "
            f"skipped={results['skipped']} failed={results['failed']}"
        )

    except Exception as e:
        console.print(f"[red]Error during metadata backfill: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("doctor")
def chatgpt_doctor():
    """Run diagnostic checks for ChatGPT export ingestion setup.

    Validates configuration, directory permissions, and Gmail authentication.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    try:
        from .chatgpt.ingest import doctor_check

        console.print("[cyan]Running ChatGPT ingestion diagnostics...[/cyan]")
        results = doctor_check(config, paths)

        # Report results
        if results["config_valid"]:
            console.print("[green]✓[/green] Configuration is valid")
        else:
            console.print("[red]✗[/red] Configuration has errors:")
            for error in results["errors"]:
                console.print(f"  [red]- {error}[/red]")

        if results["directories_writable"]:
            console.print("[green]✓[/green] All directories are writable")
        else:
            console.print("[red]✗[/red] Directory permission errors:")
            for error in results["errors"]:
                if "writable" in error:
                    console.print(f"  [red]- {error}[/red]")

        if results["obsidian_dirs_exist"]:
            console.print("[green]✓[/green] Obsidian directories exist")
        else:
            console.print("[yellow]⚠[/yellow] Obsidian directory warnings:")
            for warning in results["warnings"]:
                console.print(f"  [yellow]- {warning}[/yellow]")

        if results["gmail_auth_works"] is True:
            console.print("[green]✓[/green] Gmail authentication successful")
        elif results["gmail_auth_works"] is False:
            console.print("[red]✗[/red] Gmail authentication failed")
            for error in results["errors"]:
                if "Gmail" in error:
                    console.print(f"  [red]- {error}[/red]")
        else:
            console.print("[dim]○[/dim] Gmail authentication not tested")

        # Summary
        has_errors = bool(results["errors"])
        has_warnings = bool(results["warnings"])

        if has_errors:
            console.print(f"\n[red]Found {len(results['errors'])} error(s) that need to be fixed[/red]")
            raise typer.Exit(code=1)
        elif has_warnings:
            console.print(f"\n[yellow]Found {len(results['warnings'])} warning(s) - check Obsidian directory setup[/yellow]")
        else:
            console.print("\n[green]All checks passed![/green]")

    except Exception as e:
        console.print(f"[red]Error running diagnostics: {e}[/red]")
        raise typer.Exit(code=1)


@chatgpt_app.command("install-launchd")
def chatgpt_install_launchd():
    """Generate and install macOS LaunchAgent for automated ChatGPT export ingestion.

    Creates a plist file in ~/Library/LaunchAgents/ and prints the load command.
    Does NOT automatically load the agent - you must run the printed command manually.
    """
    import os
    import platform
    from pathlib import Path

    # Check if we're on macOS
    if platform.system() != "Darwin":
        console.print("[red]Error: launchd is only available on macOS[/red]")
        raise typer.Exit(code=1)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    try:
        # Determine paths
        node_path = Path(os.sys.executable).resolve()
        cli_path = Path(__file__).resolve()

        # Create LaunchAgents directory if it doesn't exist
        launch_agents_dir = Path.home() / "Library" / "LaunchAgents"
        launch_agents_dir.mkdir(parents=True, exist_ok=True)

        # Generate plist content
        plist_content = f'''<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0//EN">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>{config.launchd.label}</string>

    <key>ProgramArguments</key>
    <array>
        <string>{node_path}</string>
        <string>{cli_path}</string>
        <string>chatgpt</string>
        <string>ingest-latest-export</string>
    </array>

    <key>StartInterval</key>
    <integer>{config.launchd.interval_seconds}</integer>

    <key>StandardOutPath</key>
    <string>{paths.root / "logs" / "launchd_stdout.log"}</string>

    <key>StandardErrorPath</key>
    <string>{paths.root / "logs" / "launchd_stderr.log"}</string>

    <key>EnvironmentVariables</key>
    <dict>
        <key>TOTEM_VAULT</key>
        <string>{config.vault_path}</string>
    </dict>
</dict>
</plist>'''

        # Write plist file
        plist_path = launch_agents_dir / f"{config.launchd.label}.plist"
        plist_path.write_text(plist_content, encoding='utf-8')

        console.print(f"[green]Created launchd plist:[/green] {plist_path}")

        # Create logs directory
        logs_dir = paths.root / "logs"
        logs_dir.mkdir(exist_ok=True)

        console.print(f"[green]Created logs directory:[/green] {logs_dir}")

        # Print load command
        console.print("\n[cyan]To load and start the launch agent, run:[/cyan]")
        console.print(f"[bold]launchctl load -w {plist_path}[/bold]")

        console.print("\n[cyan]To unload the launch agent:[/cyan]")
        console.print(f"[bold]launchctl unload -w {plist_path}[/bold]")

        console.print("\n[cyan]To check status:[/cyan]")
        console.print(f"[bold]launchctl list | grep {config.launchd.label}[/bold]")

        console.print(f"\n[dim]Note: The agent will run every {config.launchd.interval_seconds} seconds ({config.launchd.interval_seconds // 3600} hours)[/dim]")

    except Exception as e:
        console.print(f"[red]Error creating launchd configuration: {e}[/red]")
        raise typer.Exit(code=1)


ledger_app = typer.Typer(help="Ledger commands")
app.add_typer(ledger_app, name="ledger")


@ledger_app.command("tail")
def ledger_tail(
    n: int = typer.Option(
        20,
        "--n",
        help="Number of recent events to display",
    ),
    full: bool = typer.Option(
        False,
        "--full",
        help="Show full payloads with JSON pretty-print",
    ),
):
    """Display the last N events from the ledger.

    Shows recent ledger events with timestamps, types, and payloads.
    Skips malformed lines with warnings.
    Use --full to see complete payloads with JSON formatting.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Read ledger tail
    events = read_ledger_tail(paths.ledger_file, n=n)

    if not events:
        console.print("[dim]No events in ledger[/dim]")
        return

    if full:
        # Full mode: show each event with pretty-printed JSON
        console.print(f"[bold]Last {len(events)} Ledger Event(s)[/bold]\n")
        for i, event in enumerate(events, 1):
            console.print(f"[cyan]Event {i}/{len(events)}[/cyan]")
            console.print(f"  [dim]Event ID:[/dim]    {event.event_id}")
            console.print(f"  [dim]Run ID:[/dim]      {event.run_id}")
            console.print(f"  [dim]Timestamp:[/dim]   {event.ts.strftime('%Y-%m-%d %H:%M:%S')} UTC")
            console.print(f"  [dim]Event Type:[/dim]  [magenta]{event.event_type}[/magenta]")
            console.print(f"  [dim]Capture ID:[/dim]  {event.capture_id or '-'}")
            console.print(f"  [dim]Payload:[/dim]")
            
            import json
            payload_json = json.dumps(event.payload, indent=2)
            for line in payload_json.split('\n'):
                console.print(f"    {line}")
            console.print()
    else:
        # Table mode: compact view with truncation
        table = Table(title=f"Last {len(events)} Ledger Event(s)")
        table.add_column("Timestamp (UTC)", style="cyan", no_wrap=True)
        table.add_column("Event Type", style="magenta")
        table.add_column("Capture ID", style="yellow")
        table.add_column("Payload", style="dim")

        for event in events:
            # Format timestamp with UTC indicator
            ts_str = event.ts.strftime("%Y-%m-%d %H:%M:%S")
            
            # Format capture ID
            capture_id_str = event.capture_id[:8] + "..." if event.capture_id else "-"
            
            # Format payload (truncate if too long)
            payload_str = str(event.payload)
            if len(payload_str) > 60:
                payload_str = payload_str[:57] + "..."
            
            table.add_row(ts_str, event.event_type, capture_id_str, payload_str)

        console.print(table)


@app.command()
def route(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for inbox folder (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        20,
        "--limit",
        "-l",
        help="Maximum number of captures to process",
    ),
    engine: str = typer.Option(
        "auto",
        "--engine",
        "-e",
        help="Routing engine: 'rule', 'llm', 'hybrid', or 'auto' (default: auto - hybrid if API key present, else rule)",
    ),
    llm_engine: str = typer.Option(
        "auto",
        "--llm-engine",
        help="LLM engine for llm/hybrid: 'fake', 'openai', 'anthropic', or 'auto' (default: auto)",
    ),
    no_short_circuit: bool = typer.Option(
        False,
        "--no-short-circuit",
        help="Hybrid mode: always call LLM even if rule confidence is high (for A/B testing)",
    ),
    bypass_arbiter: bool = typer.Option(
        False,
        "--bypass-arbiter",
        help="Skip IntentArbiter step",
    ),
):
    """Route captures using rule-based, LLM, hybrid routing, or IntentArbiter.
    
    Reads raw captures from 00_inbox/YYYY-MM-DD/, applies routing logic,
    and writes outputs to either routed/ or review_queue/ based on confidence.
    
    By default, IntentArbiter is run first to classify intent and route to downstream agents.
    Use --bypass-arbiter to skip this and use purely the legacy routing engines.
    
    Engine modes (legacy):
    - rule: Deterministic keyword-based heuristics only
    - llm: LLM-based classification only
    - hybrid: Rule first, LLM fallback if rule confidence < threshold
    - auto: hybrid if API key present, else rule
    
    Use --no-short-circuit to force hybrid mode to always call LLM (for A/B testing).
    """
    from .llm.router import has_llm_api_key
    
    # Validate engine option
    valid_engines = ["rule", "llm", "hybrid", "auto"]
    if engine not in valid_engines:
        console.print(f"[red]Error: Invalid engine '{engine}'. Must be one of: {', '.join(valid_engines)}[/red]")
        raise typer.Exit(code=1)

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine effective engine and display info
    effective_engine = engine
    if engine == "auto":
        if has_llm_api_key():
            effective_engine = "hybrid"
            console.print("[dim]Auto-detected API key: using hybrid engine[/dim]")
        else:
            effective_engine = "rule"
            console.print("[dim]No API key found: using rule engine[/dim]")
    else:
        console.print(f"[dim]Using {engine} engine[/dim]")
    
    # Display no-short-circuit mode
    if no_short_circuit:
        if effective_engine in ("hybrid", "auto"):
            console.print("[yellow]--no-short-circuit: LLM will always be called (A/B testing mode)[/yellow]")
        else:
            console.print("[dim]Note: --no-short-circuit only affects hybrid mode[/dim]")
    
    # Check if LLM is requested but no API key
    if effective_engine in ("llm", "hybrid") and llm_engine != "fake" and not has_llm_api_key():
        if llm_engine == "auto":
            console.print("[dim]No API key found: using fake LLM router[/dim]")
        else:
            console.print(f"[red]Error: LLM engine '{llm_engine}' requested but no API key found[/red]")
            console.print("[yellow]Set OPENAI_API_KEY or ANTHROPIC_API_KEY, or use --llm-engine fake[/yellow]")
            raise typer.Exit(code=1)

    # Load vault configuration (already loaded above, but needed again for some reason?)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Find inbox date folder
    inbox_date_folder = paths.inbox_date_folder(date_str)
    
    if not inbox_date_folder.exists():
        console.print(f"[yellow]No inbox folder found for {date_str}[/yellow]")
        console.print(f"[dim]Expected: {inbox_date_folder}[/dim]")
        return

    # Find all raw capture files (exclude .meta.json files)
    all_files = []
    for file_path in inbox_date_folder.iterdir():
        if file_path.is_file() and not file_path.name.endswith(".meta.json"):
            all_files.append(file_path)
    
    if not all_files:
        console.print(f"[yellow]No capture files found in {inbox_date_folder.name}/[/yellow]")
        return

    # Sort by modification time (newest first)
    all_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    
    # Apply limit
    files_to_process = all_files[:limit]
    
    if len(all_files) > limit:
        console.print(f"[dim]Found {len(all_files)} captures, processing {limit} (use --limit to change)[/dim]\n")
    else:
        console.print(f"[dim]Found {len(files_to_process)} capture(s) to process[/dim]\n")

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Process each capture
    results = []
    routed_count = 0
    review_count = 0

    for raw_file in files_to_process:
        # Find corresponding meta file
        meta_file = raw_file.with_suffix(raw_file.suffix + ".meta.json")
        
        if not meta_file.exists():
            console.print(f"[yellow]Warning: No meta file for {raw_file.name}, skipping[/yellow]")
            continue

        try:
            # Process routing with specified engine
            output_path, was_routed = process_capture_routing(
                raw_file_path=raw_file,
                meta_file_path=meta_file,
                vault_root=paths.root,
                config=config,
                ledger_writer=ledger_writer,
                date_str=date_str,
                engine=effective_engine,
                llm_engine=llm_engine,
                no_short_circuit=no_short_circuit,
            )
            
            # Integrate IntentArbiter if not bypassed
            if not bypass_arbiter:
                try:
                    # Initialize Arbiter
                    arbiter = IntentArbiterAgent(
                        ledger_writer=ledger_writer,
                        vault_root=paths.root,
                        llm_engine="auto" if has_llm_api_key() else "fake"
                    )
                    
                    # Read text again (it was read inside process_capture_routing too, but that's fine)
                    text_content = raw_file.read_text(encoding="utf-8")
                    
                    console.print(f"[dim]Running IntentArbiter on {raw_file.name}...[/dim]")
                    arbiter.run(text_content)
                    
                except Exception as e:
                    console.print(f"[red]IntentArbiter failed for {raw_file.name}: {e}[/red]")
            
            # Read the output to get details for display
            import json
            output_data = json.loads(output_path.read_text(encoding="utf-8"))
            
            results.append({
                "capture_id": output_data["capture_id"],
                "route": output_data["route_label"],
                "confidence": output_data["confidence"],
                "was_routed": was_routed,
                "output_path": output_path.relative_to(paths.root),
            })
            
            if was_routed:
                routed_count += 1
            else:
                review_count += 1

        except Exception as e:
            console.print(f"[red]Error processing {raw_file.name}: {e}[/red]")
            continue

    # Display results table
    if results:
        table = Table(title=f"Routing Results for {date_str} (engine: {effective_engine})")
        table.add_column("Capture ID", style="cyan")
        table.add_column("Route", style="magenta")
        table.add_column("Confidence", style="yellow")
        table.add_column("Destination", style="green")

        for result in results:
            capture_id_short = result["capture_id"][:8] + "..."
            confidence_str = f"{result['confidence']:.2f}"
            destination = "routed" if result["was_routed"] else "review_queue"
            
            table.add_row(
                capture_id_short,
                result["route"],
                confidence_str,
                destination
            )

        console.print(table)
        console.print()
        console.print(f"[bold green]Summary:[/bold green] {routed_count} routed, {review_count} flagged for review")
    else:
        console.print("[yellow]No captures were processed[/yellow]")


@app.command()
def intent(
    text: str = typer.Option(
        ...,
        "--text",
        "-t",
        help="Input text to classify and route",
    ),
):
    """Classify and route a single input using IntentArbiter."""
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)
    ledger_writer = LedgerWriter(paths.ledger_file)
    
    arbiter = IntentArbiterAgent(
        ledger_writer=ledger_writer,
        vault_root=paths.root,
        llm_engine="auto"
    )
    
    console.print(f"[bold]Input:[/bold] {text}")
    
    # Run arbiter (Classify -> Log -> Route -> Agent.run)
    arbiter.run(text)


@app.command()
def distill(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for processing (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        20,
        "--limit",
        "-l",
        help="Maximum number of routed items to process",
    ),
    engine: str = typer.Option(
        "auto",
        "--engine",
        "-e",
        help="LLM engine: 'fake', 'real', 'openai', 'anthropic', or 'auto' (default: auto)",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview distillation without writing canon files",
    ),
):
    """Distill routed captures using LLM and apply canon writes.

    Reads from 10_derived/routed/YYYY-MM-DD/, processes through LLM distillation,
    and writes results to distill/, daily notes, todo, and entities.
    All writes are append-only with undo support via 'totem undo'.

    Use --dry-run to preview what would be written without applying changes.
    """
    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Get LLM client
    try:
        llm_client = get_llm_client(engine)
        console.print(f"[dim]Using LLM engine: {llm_client.engine_name}[/dim]")
        if llm_client.provider_model:
            console.print(f"[dim]Provider/model: {llm_client.provider_model}[/dim]")
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)

    # Load routed items
    routed_items = load_routed_items(paths, date_str, limit=limit)
    
    if not routed_items:
        console.print(f"[yellow]No routed items found for {date_str}[/yellow]")
        console.print(f"[dim]Expected: {paths.routed_date_folder(date_str)}[/dim]")
        return

    if dry_run:
        console.print(f"[yellow]DRY-RUN MODE[/yellow] — Found {len(routed_items)} routed item(s) to preview\n")
    else:
        console.print(f"[dim]Found {len(routed_items)} routed item(s) to process[/dim]\n")

    # Initialize ledger writer (only used in non-dry-run mode)
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Process each routed item
    results = []
    for item in routed_items:
        capture_id = item.get("capture_id", "unknown")
        capture_id_short = capture_id[:8] + "..."
        
        try:
            console.print(f"[cyan]Processing:[/cyan] {capture_id_short}")
            
            if dry_run:
                # Dry-run mode: generate but don't write canon files
                distill_result, would_apply, distill_path = process_distillation_dry_run(
                    routed_item=item,
                    llm_client=llm_client,
                    vault_paths=paths,
                    date_str=date_str,
                )
                
                results.append({
                    "capture_id": capture_id,
                    "confidence": distill_result.confidence,
                    "summary": distill_result.summary[:50] + "..." if len(distill_result.summary) > 50 else distill_result.summary,
                    "tasks_count": len(distill_result.tasks),
                    "entities_count": len(distill_result.entities),
                    "would_modify": len(would_apply),
                    "distill_path": distill_path,
                    "would_apply": would_apply,
                })
                
                console.print(f"  [green]+[/green] Distilled (confidence: {distill_result.confidence:.2f})")
                console.print(f"    Distill artifact: [dim]{distill_path}[/dim]")
                console.print(f"    [yellow]Would write to {len(would_apply)} file(s):[/yellow]")
                for af in would_apply:
                    console.print(f"      - {af.path}")
            else:
                # Normal mode: write canon files
                distill_result, write_record = process_distillation(
                    routed_item=item,
                    llm_client=llm_client,
                    vault_paths=paths,
                    ledger_writer=ledger_writer,
                    date_str=date_str,
                )
                
                results.append({
                    "capture_id": capture_id,
                    "write_id": write_record.write_id,
                    "confidence": distill_result.confidence,
                    "summary": distill_result.summary[:50] + "..." if len(distill_result.summary) > 50 else distill_result.summary,
                    "tasks_count": len(distill_result.tasks),
                    "entities_count": len(distill_result.entities),
                    "modified_files": len(write_record.applied_files),
                })
                
                console.print(f"  [green]+[/green] Distilled (confidence: {distill_result.confidence:.2f})")
                console.print(f"    Write ID: [yellow]{write_record.write_id[:8]}...[/yellow]")
            
        except Exception as e:
            console.print(f"  [red]x Error: {e}[/red]")
            continue

    # Summary
    console.print()
    if results:
        if dry_run:
            table = Table(title=f"Dry-Run Preview — {date_str}")
            table.add_column("Capture", style="cyan")
            table.add_column("Conf", style="green")
            table.add_column("Tasks", style="magenta")
            table.add_column("Entities", style="blue")
            table.add_column("Would Write", style="yellow")
            
            for r in results:
                table.add_row(
                    r["capture_id"][:8] + "...",
                    f"{r['confidence']:.2f}",
                    str(r["tasks_count"]),
                    str(r["entities_count"]),
                    str(r["would_modify"]),
                )
            
            console.print(table)
            console.print()
            console.print(f"[bold yellow]DRY-RUN Summary:[/bold yellow] {len(results)} item(s) would be distilled")
            console.print("[dim]Distill artifacts were created. Canon files were NOT modified.[/dim]")
            console.print("[dim]Run without --dry-run to apply changes.[/dim]")
        else:
            table = Table(title=f"Distillation Results — {date_str}")
            table.add_column("Capture", style="cyan")
            table.add_column("Write ID", style="yellow")
            table.add_column("Conf", style="green")
            table.add_column("Tasks", style="magenta")
            table.add_column("Entities", style="blue")
            
            for r in results:
                table.add_row(
                    r["capture_id"][:8] + "...",
                    r["write_id"][:8] + "...",
                    f"{r['confidence']:.2f}",
                    str(r["tasks_count"]),
                    str(r["entities_count"]),
                )
            
            console.print(table)
            console.print()
            console.print(f"[bold green]Summary:[/bold green] {len(results)} item(s) distilled")
            console.print("[dim]Use 'totem undo --write-id <ID>' to reverse any write[/dim]")
    else:
        console.print("[yellow]No items were successfully distilled[/yellow]")


@app.command()
def undo(
    write_id: str = typer.Option(
        ...,
        "--write-id",
        "-w",
        help="Write ID (UUID) to undo",
    ),
):
    """Undo a canon write by removing inserted blocks.

    Reverses the append-only writes from a distillation operation.
    The write ID can be found in the distillation output or ledger.
    """
    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Initialize ledger writer
    ledger_writer = LedgerWriter(paths.ledger_file)

    try:
        console.print(f"[cyan]Undoing write:[/cyan] {write_id}")
        
        modified_files = undo_canon_write(
            write_id=write_id,
            vault_paths=paths,
            ledger_writer=ledger_writer,
        )
        
        console.print()
        console.print("[bold green]Undo successful![/bold green]")
        
        if modified_files:
            console.print("[dim]Modified files:[/dim]")
            for path in modified_files:
                console.print(f"  - {path}")
        else:
            console.print("[yellow]No files were modified[/yellow]")
        
        console.print()
        console.print("[dim]Note: entities.json changes require manual review[/dim]")
        
    except FileNotFoundError as e:
        console.print(f"[red]Error: {e}[/red]")
        console.print("[dim]Check the write ID is correct (from distill output or ledger)[/dim]")
        raise typer.Exit(code=1)
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)
    except Exception as e:
        console.print(f"[red]Unexpected error: {e}[/red]")
        raise typer.Exit(code=1)


@app.command()
def review(
    date: str = typer.Option(
        None,
        "--date",
        "-d",
        help="Date for proposals (YYYY-MM-DD, default: today)",
    ),
    limit: int = typer.Option(
        None,
        "--limit",
        "-l",
        help="Maximum number of proposals to review",
    ),
    queue_path: str = typer.Option(
        None,
        "--queue",
        "-q",
        help="Override path to review queue file",
    ),
    dry_run: bool = typer.Option(
        False,
        "--dry-run",
        help="Preview mode: show proposals without writing artifacts",
    ),
):
    """Interactive review loop for proposed artifacts.

    Single-keystroke review interface:
      [A]pprove  - Write proposal to canon
      [V]eto     - Discard proposal (logs learning event)
      [C]orrect  - Override with corrected artifact
      [D]efer    - Keep in queue for later
      [Q]uit     - Exit review session

    Philosophy: You never file, only judge. No silent writes.
    """
    # Determine date string (today if not provided)
    if date:
        date_str = date
    else:
        date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Get global vault path
    vault_path = get_global_vault_path()

    # Load vault configuration
    config = TotemConfig.from_env(cli_vault_path=vault_path)
    paths = VaultPaths.from_config(config)

    # Check if vault exists
    if not paths.system.exists():
        console.print(f"[red]Error: Vault not initialized at {config.vault_path}[/red]")
        console.print("[yellow]Run 'totem init' first[/yellow]")
        raise typer.Exit(code=1)

    # Determine queue path
    if queue_path:
        review_queue_file = Path(queue_path)
    else:
        review_queue_file = paths.review_queue_file

    # Show mode
    if dry_run:
        console.print("[yellow]DRY-RUN MODE[/yellow] — No artifacts will be written\n")
    
    console.print(f"[dim]Date: {date_str}[/dim]")
    console.print(f"[dim]Queue: {review_queue_file}[/dim]\n")

    # Load or create proposals
    console.print("[dim]Loading proposals...[/dim]")
    proposals = load_or_create_proposals(paths, date_str)
    
    if not proposals:
        console.print("[yellow]No proposals to review.[/yellow]")
        console.print(f"[dim]Run 'totem distill --date {date_str}' first to generate proposals.[/dim]")
        return

    console.print(f"[green]Found {len(proposals)} proposal(s) to review.[/green]\n")

    # Initialize components
    review_queue = ReviewQueue(review_queue_file)
    learning_logger = LearningEventLogger(paths.review_events_file)
    ledger_writer = LedgerWriter(paths.ledger_file)

    # Create and run session
    session = ReviewSession(
        review_queue=review_queue,
        learning_logger=learning_logger,
        vault_paths=paths,
        ledger_writer=ledger_writer,
        dry_run=dry_run,
        output_fn=print,  # Use print for terminal output
    )

    try:
        summary = session.run(limit=limit)
    except KeyboardInterrupt:
        console.print("\n[yellow]Review session interrupted.[/yellow]")
        summary = session._get_summary()

    # Display summary
    console.print("\n" + "=" * 40)
    console.print("[bold]Review Session Summary[/bold]")
    console.print("=" * 40)
    console.print(f"  Approved:  {summary['approved']}")
    console.print(f"  Vetoed:    {summary['vetoed']}")
    console.print(f"  Corrected: {summary['corrected']}")
    console.print(f"  Deferred:  {summary['deferred']}")
    console.print(f"  [bold]Total:     {summary['total']}[/bold]")
    
    if dry_run:
        console.print("\n[yellow]DRY-RUN: No changes were made.[/yellow]")
    else:
        console.print(f"\n[dim]Learning events logged to: {paths.review_events_file}[/dim]")


@app.command()
def version():
    """Show Totem OS version."""
    from . import __version__
    console.print(f"Totem OS v{__version__}")


def main():
    """Entry point for the CLI."""
    # Handle --version before Typer processing
    if len(sys.argv) == 2 and sys.argv[1] in ("--version", "-v"):
        from . import __version__
        console.print(f"Totem OS v{__version__}")
        sys.exit(0)

    app()


if __name__ == "__main__":
    main()
